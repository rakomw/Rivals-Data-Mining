\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
% \usepackage{natbib}  % Uncomment if using .bib file with bibtex
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% Single spacing
\usepackage{setspace}
\singlespacing

% Title information
\title{Marvel Rivals Win Prediction and Hero Recommendation}
\author{
    Robert Moore \\
    \texttt{robert.moore.593@my.csun.edu}
    \and
    Ken Douglas \\
    \texttt{kenneth.douglas.994@my.csun.edu}
    \and
    Behrouz Barati B \\
    \texttt{b3h@me.com}
}
\date{December 12, 2025}

\begin{document}

\maketitle
% ============================================================================
\section{Introduction}
Marvel Rivals is a third-person hero shooter where players battle in teams of 6 to accomplish various objectives, such as capturing checkpoints or safely escorting a moving object. As they face the enemy team and achieve or fail their objectives, each player accumulates statistics including damage dealt, KO's, Deaths, and more, which are gathered by integrated trackers and transmitted to online leaderboards along with the final match result. These statistics are the only objective record of a match, but their relationship to the final outcome is indirect and often unclear, making it difficult for, eg, an Esports coach to assess a prospective player's skill on the basis of their match history. Being able to make decisions off of simple statistics from a game is huge for the industry and a growing need in a data driven society. We sought to develop models that could accurately determine the outcome of a match based on these statistics. 

% ============================================================================

% Describe the problem
\subsection{Problem Description}
  
The goal of my contribution was to predict the winner of a Marvel Rivals match using team composition, player performance metrics, and structural game features. Rather than relying solely on raw damage or kill counts, the approach focuses on composition, synergy, consistency, and tempo—similar to how team strength is evaluated in competitive play. My work centered on transforming raw, inconsistent JSON match logs into a structured dataset suitable for modeling, and extending the resulting model into a hero recommendation system to support draft decision-making.

The dataset consists of over 310{,}000 cleaned matches, each containing two teams whose players may swap heroes and roles mid-match. Significant preprocessing was required due to inconsistent time formats, zero-minute players, and irregular team sizes. A key component of the cleaning pipeline was randomized team flipping: because Team One appears to win more frequently due to logging structure, matches were flipped with 50\% probability to remove positional bias and force the model to learn outcome-driving features rather than shortcuts.

After cleaning, exploratory analysis of role usage and per-minute performance informed the feature engineering process and directly influenced both the neural network design and the hero recommendation system.

% Describe the data
\subsection{Data Description}
We obtained a collection of match data by crawling the website rivalsmeta.com, which collates statistics from official trackers integrated into the game's various platforms (PC, PlayStation, and Xbox). Our crawler is written in Python, with the Playwright library being used to navigate the site and interact with dynamic page elements, while the BeautifulSoup library was used to parse the HTML into an easily-navigable DOM tree from which we could extract the relevant data. The crawler followed a breadth-first approach to locate new pages to scrape–it began with a few hand-selected public profiles of players near the top of the leaderboards, from which it pulled the data for every match they had played during the target time period. The data for each of these matches includes the identifiers of the player's teammates and opponents, all of which were added to a queue to be processed in the same manner as the original profiles. The process was repeated, while tracking which players' profiles had already been visited, eventually yielding data on 330,914 matches, including 458,565 unique players. Each match contains two teams, with player-level statistics such as damage, healing, deaths, hero usage, and rank. Players may swap heroes mid-match, resulting in variable hero counts per player. The target variable is a binary label indicating whether Team One or Team Two won the match. 


Describe the dataset(s) used in this project. Include information such as:
\begin{itemize}
    \item Source of the data
    \item Number of samples and features
    \item Types of variables (categorical, numerical, etc.)
    \item Target variable (if applicable)
\end{itemize}

% Data preparation
\subsection{Data Preparation}
The first stage of my contribution was building a cleaning pipeline capable of preparing 330,914 matches for modeling. The major steps included:
\begin{itemize}
	\item 	Time normalization: Converting formats like “8.5m” or “45s” into consistent numeric minutes.
	\item 	Filtering invalid players: Removing players with 0 minutes played, since they introduce noise into per-minute calculations.
	\item	Enforcing team size: Some matches listed more than 6 players on a team; I kept only the 6 highest-minute players.
	\item 	Role mapping: Every hero was mapped into Tank, DPS, or Support, allowing role-share features.
\item Randomized flipping: With 50\% probability, I flipped the entire match (team\_one \(\leftrightarrow\) team\_two, scores, and winner).
This prevents the model from learning position-based shortcuts.
	\item 	Leakage prevention: Final score and round count were explicitly excluded from model input. 
\end{itemize}

By the end, the cleaned dataset captured a realistic competitive match environment with consistent structures across all matches. -KD


Describe the data preprocessing steps:
\begin{itemize}
    \item Handling missing values
    \item Feature encoding
    \item Normalization/Standardization
    \item Feature selection or engineering
    \item Train/test split strategy
\end{itemize}

% Data visualization
\subsection{Data Visualization}
Include visualizations that help understand the data distribution and patterns.

After cleaning, I explored distributions of role minutes, per-minute efficiency, and hero usage. These visualizations helped confirm that Tanks, DPS, and Supports exhibit distinct performance patterns, and guided the feature engineering choices used in the neural network.

% Example figure placeholder
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/data_visualization.png}
%     \caption{Description of the visualization}
%     \label{fig:data_viz}
% \end{figure}

% ============================================================================
\section{Related Work}
% ============================================================================

\paragraph{Ken Douglas:}
Prior work has shown that match outcomes can be predicted accurately using engineered player- and team-level features rather than raw game logs, reinforcing the importance of feature engineering before model training~\cite{ref1}. The effectiveness of neural models combined with normalized performance metrics motivated my use of per-minute statistics, carry-style aggregates, and a sigmoid-based neural network output for win probability prediction.

Other studies have demonstrated that team composition alone, independent of in-game events, has a measurable impact on win probability~\cite{ref6}. While those approaches rely on search-based optimization, I adapt the core idea by using a trained neural network to dynamically evaluate and rank candidate hero picks during the draft phase.
\paragraph{Robert Moore}
Discuss another relevant paper or body of work. Explain the methodology used, the results obtained, and how it relates to your project.

\paragraph{}
Gradient boosting and ensemble tree methods have demonstrated strong performance in esports match outcome prediction. Xiao et al.~\cite{xiao2021wpgbdt} proposed WP-GBDT, achieving 89.97\% AUC using gradient boosting decision trees with feature engineering on video game logs at IEEE BigData 2021. Akhmedov and Phan~\cite{akhmedov2021dota2} applied machine learning to Dota 2 match prediction, comparing Linear Regression (82\%), Neural Networks (88\%), and LSTM (93\%) using real-time game state data. Gourdeau and Archambault~\cite{gourdeau2020hero} developed discriminative neural networks for hero selection in MOBA games, demonstrating that team composition significantly impacts match outcomes. Our work extends these approaches by applying Random Forest, XGBoost, and CatBoost to Marvel Rivals and introduces a Retrieval-Augmented Generation (RAG) pipeline for natural language exploration of match statistics.

% ============================================================================
\section{Methods}
% ============================================================================

Describe the methods and algorithms used to solve the problem.

\subsection{Method 1: Neural Network(KD)}

\subsection{Cleaning Pipeline}

The first stage of my contribution was building a cleaning pipeline capable of preparing over 310{,}000 matches for modeling. The raw Marvel Rivals JSON data was inconsistent and not directly usable, so several preprocessing steps were required.

The major steps in the cleaning pipeline were as follows:
\begin{itemize}
    \item \textbf{Time normalization:} All time formats (e.g., ``8.5m'' or ``45s'') were converted into consistent numeric minute values.
    \item \textbf{Filtering invalid players:} Players with zero minutes played were removed, as they introduce noise into per-minute performance calculations.
    \item \textbf{Enforcing team size:} Some matches listed more than six players on a team; in these cases, only the six players with the highest total minutes were retained.
    \item \textbf{Role mapping:} Each hero was mapped to one of three roles---Tank, DPS, or Support---enabling role-based feature engineering.
    \item \textbf{Randomized team flipping:} With 50\% probability, the entire match was flipped (team\_one $\leftrightarrow$ team\_two), including scores and winner labels. This prevents the model from learning positional shortcuts such as ``Team One wins more often.''
    \item \textbf{Leakage prevention:} Final scores and round counts were explicitly excluded from the neural network input features.
\end{itemize}

After these steps, the cleaned dataset represented a realistic competitive environment with consistent structure across all matches.

\subsection{Feature Engineering}

The neural network relies heavily on engineered features that represent team strength, synergy, tempo, and performance distribution. The engineered feature set includes:

\begin{itemize}
    \item \textbf{Raw team totals:} Damage dealt, healing, damage taken, kills, assists, final hits, and deaths.
    \item \textbf{Per-minute efficiency metrics:} Damage per minute (DPM), healing per minute (HPM), and damage taken per minute, which normalize performance by playtime.
    \item \textbf{Carry score:}
    \[
    \text{Carry Score} = \frac{\text{damage} + \text{taken} + \text{healed} + \text{final hits}}{\text{deaths} + 1}
    \]
    This metric captures high-impact, low-death players and helps identify which team has a dominant ``carry,'' regardless of role.
    \item \textbf{Role composition:} Total and percentage minutes played by Tanks, DPS, and Supports.
    \item \textbf{Player consistency features:} Hero swap count, number of heroes played, and main-hero usage share.
    \item \textbf{Synergy and team-up features:} Counts of known hero team-up combinations (e.g., Invisible Woman and Human Torch) to capture composition synergy.
    \item \textbf{Statistical distributions:} Mean, maximum, and standard deviation values for deaths, DPM, and carry scores across players on each team.
\end{itemize}

Together, these features create a detailed statistical fingerprint of each team, capturing both raw performance and compositional structure.

\subsection{Neural Network Model}

The predictive model is a two-hidden-layer neural network implemented entirely in NumPy. The architecture consists of:
\begin{itemize}
    \item An input layer with approximately 94 engineered features.
    \item A first hidden layer with 16 neurons using the Leaky ReLU activation function.
    \item A second hidden layer with 16 neurons using the Leaky ReLU activation function.
    \item An output layer with a sigmoid activation function that produces the probability of Team One winning the match.
\end{itemize}

This architecture balances expressive power with computational efficiency. The data exhibits strong nonlinear relationships---such as role synergy, hero swapping behavior, and carry distributions---but deeper or wider networks increased training time without improving generalization. Leaky ReLU was chosen to maintain stable gradients even when feature values are sparse or negative.

The training configuration was as follows:
\begin{itemize}
    \item \textbf{Loss function:} Binary Cross-Entropy
    \item \textbf{Optimization:} Mini-batch Gradient Descent
    \item \textbf{Batch size:} 2048
    \item \textbf{Learning rate:} 0.001
    \item \textbf{Early stopping patience:} 5 epochs
    \item \textbf{Standardization:} Feature means and standard deviations computed using the training split only
\end{itemize}

This configuration resulted in stable and efficient convergence on the full dataset.

\subsection{Hero Recommendation System}

After training the neural network, I extended the project with a hero recommendation system designed to simulate draft decision-making.

The recommendation process operates as follows:
\begin{enumerate}
    \item The user inputs up to five friendly heroes and up to six enemy heroes.
    \item Typical per-hero statistics are generated using aggregate statistics computed from the dataset.
    \item For each hero not yet selected, the system simulates adding that hero to the friendly team as the sixth pick.
    \item Each simulated team composition is encoded into the same feature vector used during training.
    \item The trained neural network evaluates the predicted win probability for each candidate composition.
    \item The system returns the top three recommended heroes for each role: Tank, DPS, and Support.
\end{enumerate}

This recommendation system transforms the neural network from a passive predictor into an active decision-support tool, allowing users to explore how different hero picks influence match outcomes.

\subsection{SVM}
A Support Vector Machine (SVM) is a supervised machine learning method for classification which finds the hyperplane that divides the data into target classes while maximizing the margin from the boundary plane to the nearest points. As many datasets are not linearly separable in their original feature space, it is common practice to use a kernel function which implicitly maps the data into a higher dimension space in which the boundary is a hyperplane. The Radial Basis Function (RBF) kernel is a popular choice, mapping the data into an infinite-dimensional space and frequently enabling linear separation of the data. We began with this approach, but as we gathered more data the time to fit the model began to grow impractical (over 1 hour with 100,000 samples, implying a full day of training to fit to the final dataset). Therefore, we used the RBFSampler kernel approximation implemented in scikit-learn, which approximates a RBF feature map using random Fourier features as described in \cite{randomkitchensinks}, and fit a linear SVM classifier to the transformed data. This achieved nearly identical results as the fully kernelized approach on smaller subsets of our data, but was much more computationally efficient, being able to fit to the final dataset of over 300k items in roughly 1 minute. The hyperparameters $\gamma$ and n\_components of the RBF sampler were tuned via cross-validation, with peak performance being found with $\gamma$ set to scale inversely to the number of features and the calculated variance in the original data, and $n\_components = 48$.

\subsection{Individual Player Models}
We also developed SVM models for predicting match outcomes and season-long player performance based on individual player's match statistics. The first of these was a linear SVM classifier like the one described above, but only operating on a single player's data rather than the entire team's. This approach reached an accuracy of 67\% in classifying matches as a win or loss, compared to the 89\% accuracy of the model given the full team's data. This result emphasizes the team-based nature of the game, showing that in as many as one in three games a player could find their team losing despite individually performing very well, or winning while undercontributing to the team. We also trained SVM-based regression models in an attempt to predict a player's season-long Win/Loss ratio, peak ELO rating, and average ELO rating, based only on their average match statistics. Despite trying various adjustments, none of these models ultimately performed significantly better than random--$R^2$ values for each ranged from $0.06-0.14$. We believe it may be possible to construct models that better reflect player skill and can predict these outcomes, but we also believe that this result could demonstrate the efficacy of the online matchmaking system: a player may win many matches at first, creating an impressive statistical record and a high W/L ratio, but as their ELO rises and they are paired with increasingly skilled opponents, the expected tendency is for their performance to stabilize and eventually converge to 50/50. It is notable that the model predicting season-long average ELO was the most effective of the three, indicating that there is still some difference in performance between a high-ranked player and a low-ranked player even when their Win/Loss ratios have converged to a similar level. 

\subsection{Random Forest}
Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of their predictions. Each tree is trained on a bootstrap sample of the data with a random subset of features at each split, reducing overfitting through decorrelation. The final prediction is:
\[
\hat{y} = \text{mode}\{h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_T(\mathbf{x})\}
\]
where $h_t(\mathbf{x})$ is the prediction of tree $t$ and $T=100$ is the number of trees.

\textbf{Configuration:} We used 100 estimators with default scikit-learn parameters. The model was trained on 105 features: 19 team statistics (kills, deaths, damage, healing, etc.) and 86 hero composition features (43 heroes $\times$ 2 teams encoded as binary presence indicators). Training used an 80/20 split with 5-fold stratified cross-validation.

\subsection{XGBoost}
XGBoost (Extreme Gradient Boosting) is a sequential boosting algorithm where each tree corrects errors from previous trees using second-order gradient optimization. It employs regularization (L1/L2) and column subsampling to prevent overfitting. The objective function minimized is:
\[
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{t=1}^{T} \Omega(f_t), \quad \Omega(f) = \gamma T + \frac{1}{2}\lambda \|w\|^2
\]
where $l$ is the logistic loss, $\Omega$ is the regularization term, $\gamma$ penalizes tree complexity, and $\lambda$ is L2 regularization on leaf weights $w$.

\textbf{Configuration:} We used 100 boosting rounds with learning rate 0.1, max depth 6, and 80\% column subsampling per tree. The objective function was binary logistic regression with evaluation metric set to AUC-ROC.

\subsection{CatBoost}
CatBoost (Category Boosting) extends gradient boosting with ordered boosting to reduce prediction shift and native handling of categorical features. It uses symmetric trees and applies L2 regularization on leaf values for stability. Ordered boosting computes unbiased gradients using a random permutation $\sigma$ of training examples:
\[
g_i = \frac{\partial L(y_i, F^{(\sigma, i-1)}(x_i))}{\partial F}, \quad F^{(\sigma, i-1)} = \sum_{j: \sigma(j) < \sigma(i)} f_j
\]
where gradients for example $i$ are computed using only preceding examples in the permutation.

\textbf{Configuration:} We used 100 iterations with default learning rate, depth 6, and L2 leaf regularization of 3. CatBoost's ordered boosting reduces overfitting without extensive hyperparameter tuning.

\subsection{Retrieval-Augmented Generation (RAG)}
We implemented a RAG pipeline to enable natural language exploration of match data. The system converts match records into text documents, embeds them using \texttt{mxbai-embed-large}, and stores vectors in ChromaDB with HNSW indexing. Document retrieval uses L2 (Euclidean) distance:
\[
d(q, d) = \|\mathbf{e}_q - \mathbf{e}_d\|_2 = \sqrt{\sum_{i=1}^{n}(e_{q,i} - e_{d,i})^2}
\]
where $\mathbf{e}_q$ and $\mathbf{e}_d$ are the embedding vectors for query $q$ and document $d$.

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Document Processing:} Each match is converted to a structured text document containing team compositions, performance statistics, and outcomes.
    \item \textbf{Meta Statistics:} Aggregate statistics (averages, hero win rates, damage/kill totals) are pre-computed and prioritized in prompts for accurate aggregate queries.
    \item \textbf{Retrieval:} On query, the top 100 similar documents are retrieved via L2 distance.
    \item \textbf{Generation:} Retrieved context is combined with meta statistics and passed to \texttt{llama3.1} for response generation.
\end{itemize}

\textbf{Winner Prediction:} For winner prediction, the RAG pipeline uses semantic search to retrieve the top 100 similar matches from 5,000 indexed documents. The query is constructed from match statistics (hero compositions, kills, deaths), embedded, and matched against the ChromaDB index. The top 10 retrieved matches are included in the LLM prompt as context for prediction. Testing on 100 randomly sampled matches achieved 61\% accuracy, while preliminary tests on 1,000 samples showed improved accuracy of approximately 80\%.

% ============================================================================
\section{Evaluation}
% ============================================================================

\subsection{Evaluation Metrics}

The primary metric used to evaluate the neural network was \textbf{accuracy}, defined as the proportion of matches for which the predicted winner matched the true winner. Accuracy was chosen because the classification task is binary and balanced after applying randomized team flipping during data cleaning.

In addition to accuracy, qualitative evaluation was performed for the hero recommendation system by observing whether the predicted win probabilities aligned with known competitive patterns such as role balance, synergy, and meta compositions.

\subsection{Results}

The trained neural network achieved the following performance:
\begin{itemize}
    \item \textbf{Training Accuracy:} 0.915
    \item \textbf{Validation Accuracy:} 0.913
    \item \textbf{Test Accuracy:} 0.915
\end{itemize}

The close alignment between training, validation, and test accuracy indicates that the model generalizes well and does not overfit the dataset. Two factors were particularly important in achieving this stability: randomized team flipping, which removed positional bias, and the exclusion of final score information from the input features, which prevented target leakage.

For the hero recommendation system, qualitative testing showed that familiar meta compositions consistently resulted in higher predicted win probabilities, while role-imbalanced or anti-synergistic drafts produced lower probabilities. These observations matched intuitive expectations from competitive play and suggest that the neural network learned structural relationships such as role composition and synergy rather than relying solely on surface-level statistics. (KD)

\begin{table}[H]
    \centering
    \caption{Model Comparison (330,914 matches)}
    \label{tab:results}
    \begin{tabular}{lccc}
        \toprule
        Model & Accuracy & AUC-ROC & F1-Score \\
        \midrule
        Neural Network & \textbf{91.50\%} & -- & -- \\
        SVM & 86.00\% & -- & -- \\
        Random Forest & 89.76\% & 96.94\% & 89.59\% \\
        XGBoost & 89.97\% & 97.16\% & 89.81\% \\
        CatBoost & 89.88\% & 97.05\% & 89.72\% \\
        RAG (LLM) & 61\%-80\% & -- & -- \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{RAG Query Examples}
The RAG system successfully answers natural language queries about match statistics:
\begin{itemize}
    \item \textit{``What is the average match duration?''} $\rightarrow$ 673 seconds (11.2 minutes)
    \item \textit{``What is the average K/D ratio?''} $\rightarrow$ 2.57
    \item \textit{``What is the average damage dealt per match?''} $\rightarrow$ 173,503 total (86,751 per team)
\end{itemize}

\subsection{Discussion}

\textbf{Tree-Based Models:} All three ensemble methods achieved approximately 90\% accuracy with minimal performance differences. XGBoost achieved the highest metrics (89.97\% accuracy, 97.16\% AUC-ROC), while CatBoost trained fastest (1.76s). The high AUC-ROC scores ($>$97\%) indicate strong class separation capability. Feature importance analysis revealed that deaths, kills, and final hits were the most predictive features across all models.

\textbf{Cross-Validation:} We employed 5-fold stratified cross-validation to ensure robust evaluation. Stratification maintained class balance across folds, critical given the balanced win/loss distribution after randomized team flipping. The low standard deviations ($\pm$0.08--0.13\%) across folds indicate stable model performance regardless of data partitioning (see Appendix~\ref{sec:appendix} for detailed CV results).

\textbf{RAG System:} The RAG pipeline achieved 61\% accuracy on 100 samples and approximately 80\% on 1,000 samples using semantic search---lower than tree-based methods but demonstrating that accuracy improves with larger sample sizes. This gap highlights that while RAG excels at natural language exploration of match statistics, structured ML models remain superior for pure prediction tasks. The RAG system's value lies in enabling non-technical exploration of match patterns rather than prediction accuracy.

% Figures
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/model_comparison.png}
    \caption{Performance comparison of tree-based models.}
    \label{fig:model_comparison}
\end{figure}

% ============================================================================
\section{Conclusion}
% ============================================================================

\subsection{Summary}
Summarize the key findings of your project. Restate the problem, the methods used, and the main results.

\subsection{Future Work}
Several directions could extend this research:
\begin{itemize}
    \item \textbf{RAG Scaling:} Our RAG winner prediction showed improved accuracy with larger sample sizes (61\% on 100 samples vs.\ 80\% on 1,000 samples). With additional computational resources, testing on larger sample sizes (10,000+ matches) could further improve accuracy and better characterize the RAG pipeline's prediction ceiling.
    \item \textbf{Before Match Prediction:} Extending models to predict outcomes before a match using partial game state data.
    \item \textbf{Hero Recommendation Enhancement:} Incorporating temporal meta shifts and player-specific performance history.
\end{itemize}

% Manual references:
\begin{thebibliography}{9}

\bibitem{ref1}
C. J. A. Cordova, C. V. A. Villaceran and C. F. Peña, \textit{Predicting League of Legends Match Outcomes Through Machine Learning Models Using Past Match Player Performance,} 2024 IEEE International Conference on Computing (ICOCO), Kuala Lumpur, Malaysia, 2024, pp. 522-527, doi: 10.1109/ICOCO62848.2024.10928259.

\bibitem{xiao2021wpgbdt}
H. Xiao, Y. Liu, D. Du, and Z. Lu. (2021). \textit{WP-GBDT: An Approach for Winner Prediction using Gradient Boosting Decision Tree}. IEEE International Conference on Big Data, pp. 5765--5769. DOI: 10.1109/BigData52589.2021.9671688

\bibitem{akhmedov2021dota2}
K. Akhmedov and A. Phan. (2021). \textit{Machine Learning Models for DOTA 2 Outcomes Prediction}. arXiv preprint arXiv:2105.09953.

\bibitem{gourdeau2020hero}
D. Gourdeau and L. Archambault. (2020). \textit{Discriminative Neural Network for Hero Selection in Professional Heroes of the Storm and DOTA 2}. IEEE Transactions on Games, vol. 12, no. 4, pp. 427--436. DOI: 10.1109/TG.2020.2972463

\bibitem{randomkitchensinks}
Rahimi, Ali and Recht, Benjamin (2008) \textit{Weighted sums of random kitchen sinks: replacing minimization with randomization in learning}. Proceedings of the 22nd International Conference on Neural Information Processing Systems, pp. 1313-1320. DOI: 10.5555/2981780.2981944

\bibitem{ref6}
L. M. Costa, A. C. C. Souza and F. C. M. Souza, "An Approach for Team Composition in League of Legends using Genetic Algorithm," 2019 18th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames), Rio de Janeiro, Brazil, 2019, pp. 52-61, doi: 10.1109/SBGames.2019.00018.

\end{thebibliography}
% ============================================================================
\newpage
\appendix
\section{Appendix}
\label{sec:appendix}
% ============================================================================

\subsection{Cross-Validation Results}

Table~\ref{tab:cv_results} presents the 5-fold stratified cross-validation results for tree-based models. The low standard deviations demonstrate consistent performance across different data partitions.

\begin{table}[H]
    \centering
    \caption{5-Fold Stratified Cross-Validation Results}
    \label{tab:cv_results}
    \begin{tabular}{lcc}
        \toprule
        Model & Mean CV Accuracy & Std Dev \\
        \midrule
        Random Forest & 89.83\% & $\pm$0.08\% \\
        XGBoost & 90.01\% & $\pm$0.13\% \\
        CatBoost & 90.05\% & $\pm$0.13\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Additional Figures}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/combined_roc.png}
        \caption{ROC curves for tree-based models showing strong discrimination ability (AUC $>$ 0.97).}
        \label{fig:roc_curves}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/xgb_feature_importance.png}
        \caption{XGBoost feature importance ranking.}
        \label{fig:feature_importance}
    \end{minipage}
\end{figure}

\end{document}
