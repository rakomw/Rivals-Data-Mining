\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
% \usepackage{natbib}  % Uncomment if using .bib file with bibtex
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% Single spacing
\usepackage{setspace}
\singlespacing

% Title information
\title{Marvel Rivals Win Prediction and Hero Recommendation}
\author{
    Robert Moore \\
    \texttt{robert.moore.593@my.csun.edu}
    \and
    Ken Douglas \\
    \texttt{kenneth.douglas.994@my.csun.edu}
    \and
    Behrouz Barati B \\
    \texttt{b3h@me.com}
}
\date{December 12, 2025}

\begin{document}

\maketitle

\begin{abstract}
This project investigates predicting match outcomes in Marvel Rivals using machine learning. My contribution focuses on transforming raw match logs into a structured dataset, engineering team-level and player-level features, and training a neural network to predict match winners. Using over 310,000 cleaned matches, the model captures nonlinear relationships such as role composition, hero synergy, and performance consistency. The trained model achieves over 91\% accuracy across training, validation, and test sets. I further extended the model into a hero recommendation system that evaluates draft decisions by simulating win probabilities for potential hero picks. [why is esports big] -KD
\end{abstract}

% ============================================================================
\section{Introduction}
Marvel Rivals is a third-person hero shooter where players battle in teams of 6 to accomplish various objectives, such as capturing checkpoints or safely escorting a moving object. As they face the enemy team and achieve or fail their objectives, each player accumulates statistics including damage dealt, KO's, Deaths, and more, which are gathered by integrated trackers and transmitted to online leaderboards along with the final match result. These statistics are the only objective record of a match, but their relationship to the final outcome is indirect and often unclear, making it difficult for, eg, an Esports coach to assess a prospective player's skill on the basis of their match history. We sought to develop models that could accurately determine the outcome of a match based on these statistics -KD

% ============================================================================

% Describe the problem
\subsection{Problem Description}
  
The goal of my portion of the project was to predict the winner of a Marvel Rivals match using team compositions, player performance measures, and structural game features. In the data, it reminded me of how coaches look at team strength—not just through raw kills or damage, but through composition, synergy, consistency, and tempo. My work focused on turning messy, inconsistent JSON data into a form that a predictive model could actually understand, and then extending that model into a hero recommendation system that helps guide draft decision-making.

The dataset consisted of over 310,000 cleaned matches. Each match included two teams, each with players who could swap heroes and roles mid-match. The raw JSON was not directly usable; time formats varied (“30s”, “8.5m”), some players had 0 minutes played, and certain teams had more than six listed players. A major part of my role was designing the cleaning pipeline that resolved these inconsistencies.

A central idea in the cleaning process was randomized team flipping. In the raw data, Team One wins more frequently simply because of how the logs were structured. If left untreated, the neural network would learn that “Team One wins” instead of learning why they win. To counter this, I flipped teams, scores, and labels with 50\% probability. This forced the model to rely only on actual features, not positional bias.

Once cleaned, the dataset allowed me to visualize behavior across roles and match dynamics—from average Tank role minutes to per-minute efficiency distributions. These insights shaped the feature engineering for the neural network and guided the eventual hero recommendation system. -KD


% Describe the data
\subsection{Data Description}
We obtained a collection of match data by crawling the website rivalsmeta.com, which collates statistics from official trackers integrated into the game's various platforms (PC, PlayStation, and Xbox). Our crawler is written in Python, with the Playwright library being used to navigate the site and interact with dynamic page elements, while the BeautifulSoup library was used to parse the HTML into an easily-navigable DOM tree from which we could extract the relevant data. The crawler followed a breadth-first approach to locate new pages to scrape–it began with a few hand-selected public profiles of players near the top of the leaderboards, from which it pulled the data for every match they had played during the target time period. The data for each of these matches includes the identifiers of the player's teammates and opponents, all of which were added to a queue to be processed in the same manner as the original profiles. The process was repeated, while tracking which players' profiles had already been visited, eventually yielding data on 330,914 matches including 458,565 unique players. 
The dataset consists of over 310,000 Marvel Rivals matches collected from competitive play logs. Each match contains two teams, with player-level statistics such as damage, healing, deaths, hero usage, and rank. Players may swap heroes mid-match, resulting in variable hero counts per player. The target variable is a binary label indicating whether Team One or Team Two won the match. -KD


Describe the dataset(s) used in this project. Include information such as:
\begin{itemize}
    \item Source of the data
    \item Number of samples and features
    \item Types of variables (categorical, numerical, etc.)
    \item Target variable (if applicable)
\end{itemize}

% Data preparation
\subsection{Data Preparation}
The first stage of my contribution was building a cleaning pipeline capable of preparing 310,000 matches for modeling. The major steps included:
\begin{itemize}
	\item 	Time normalization: Converting formats like “8.5m” or “45s” into consistent numeric minutes.
	\item 	Filtering invalid players: Removing players with 0 minutes played, since they introduce noise into per-minute calculations.
	\item	Enforcing team size: Some matches listed more than 6 players on a team; I kept only the 6 highest-minute players.
	\item 	Role mapping: Every hero was mapped into Tank, DPS, or Support, allowing role-share features.
	\item 	Randomized flipping: With 50% probability, I flipped the entire match—team_one ↔ team_two, scores, and winner.
This prevents the model from learning position-based shortcuts.
	\item 	Leakage prevention: Final score and round count were explicitly excluded from model input. 
\end{itemize}

By the end, the cleaned dataset captured a realistic competitive match environment with consistent structures across all matches. -KD


Describe the data preprocessing steps:
\begin{itemize}
    \item Handling missing values
    \item Feature encoding
    \item Normalization/Standardization
    \item Feature selection or engineering
    \item Train/test split strategy
\end{itemize}

% Data visualization
\subsection{Data Visualization}
Include visualizations that help understand the data distribution and patterns.

After cleaning, I explored distributions of role minutes, per-minute efficiency, and hero usage. These visualizations helped confirm that Tanks, DPS, and Supports exhibit distinct performance patterns, and guided the feature engineering choices used in the neural network. -KD

% Example figure placeholder
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/data_visualization.png}
%     \caption{Description of the visualization}
%     \label{fig:data_viz}
% \end{figure}

% ============================================================================
\section{Related Work}
% ============================================================================

% Each group member should write one paragraph and include their name
\paragraph{Cordova, Chaim Joseph A., et al.:}
Unlike prior work focused on League of Legends, this project applies similar principles to Marvel Rivals and extends outcome prediction into a live hero recommendation system.

\paragraph{Author 2:}
Discuss another relevant paper or body of work. Explain the methodology used, the results obtained, and how it relates to your project.

\paragraph{Behrouz Barati B:}
Gradient boosting and ensemble tree methods have demonstrated strong performance in esports match outcome prediction. Xiao et al.~\cite{xiao2021wpgbdt} proposed WP-GBDT, achieving 89.97\% AUC using gradient boosting decision trees with feature engineering on video game logs at IEEE BigData 2021. Akhmedov and Phan~\cite{akhmedov2021dota2} applied machine learning to Dota 2 match prediction, comparing Linear Regression (82\%), Neural Networks (88\%), and LSTM (93\%) using real-time game state data. Gourdeau and Archambault~\cite{gourdeau2020hero} developed discriminative neural networks for hero selection in MOBA games, demonstrating that team composition significantly impacts match outcomes. Our work extends these approaches by applying Random Forest, XGBoost, and CatBoost to Marvel Rivals and introduces a Retrieval-Augmented Generation (RAG) pipeline for natural language exploration of match statistics.

% ============================================================================
\section{Methods}
% ============================================================================

Describe the methods and algorithms used to solve the problem.

\subsection{Method 1: Neural Network(KD)}

\subsection{Cleaning Pipeline}

The first stage of my contribution was building a cleaning pipeline capable of preparing over 310{,}000 matches for modeling. The raw Marvel Rivals JSON data was inconsistent and not directly usable, so several preprocessing steps were required.

The major steps in the cleaning pipeline were as follows:
\begin{itemize}
    \item \textbf{Time normalization:} All time formats (e.g., ``8.5m'' or ``45s'') were converted into consistent numeric minute values.
    \item \textbf{Filtering invalid players:} Players with zero minutes played were removed, as they introduce noise into per-minute performance calculations.
    \item \textbf{Enforcing team size:} Some matches listed more than six players on a team; in these cases, only the six players with the highest total minutes were retained.
    \item \textbf{Role mapping:} Each hero was mapped to one of three roles---Tank, DPS, or Support---enabling role-based feature engineering.
    \item \textbf{Randomized team flipping:} With 50\% probability, the entire match was flipped (team\_one $\leftrightarrow$ team\_two), including scores and winner labels. This prevents the model from learning positional shortcuts such as ``Team One wins more often.''
    \item \textbf{Leakage prevention:} Final scores and round counts were explicitly excluded from the neural network input features.
\end{itemize}

After these steps, the cleaned dataset represented a realistic competitive environment with consistent structure across all matches.

\subsection{Feature Engineering}

The neural network relies heavily on engineered features that represent team strength, synergy, tempo, and performance distribution. The engineered feature set includes:

\begin{itemize}
    \item \textbf{Raw team totals:} Damage dealt, healing, damage taken, kills, assists, final hits, and deaths.
    \item \textbf{Per-minute efficiency metrics:} Damage per minute (DPM), healing per minute (HPM), and damage taken per minute, which normalize performance by playtime.
    \item \textbf{Carry score:}
    \[
    \text{Carry Score} = \frac{\text{damage} + \text{taken} + \text{healed} + \text{final hits}}{\text{deaths} + 1}
    \]
    This metric captures high-impact, low-death players and helps identify which team has a dominant ``carry,'' regardless of role.
    \item \textbf{Role composition:} Total and percentage minutes played by Tanks, DPS, and Supports.
    \item \textbf{Player consistency features:} Hero swap count, number of heroes played, and main-hero usage share.
    \item \textbf{Synergy and team-up features:} Counts of known hero team-up combinations (e.g., Invisible Woman and Human Torch) to capture composition synergy.
    \item \textbf{Statistical distributions:} Mean, maximum, and standard deviation values for deaths, DPM, and carry scores across players on each team.
\end{itemize}

Together, these features create a detailed statistical fingerprint of each team, capturing both raw performance and compositional structure.

\subsection{Neural Network Model}

The predictive model is a two-hidden-layer neural network implemented entirely in NumPy. The architecture consists of:
\begin{itemize}
    \item An input layer with approximately 94 engineered features.
    \item A first hidden layer with 16 neurons using the Leaky ReLU activation function.
    \item A second hidden layer with 16 neurons using the Leaky ReLU activation function.
    \item An output layer with a sigmoid activation function that produces the probability of Team One winning the match.
\end{itemize}

This architecture balances expressive power with computational efficiency. The data exhibits strong nonlinear relationships---such as role synergy, hero swapping behavior, and carry distributions---but deeper or wider networks increased training time without improving generalization. Leaky ReLU was chosen to maintain stable gradients even when feature values are sparse or negative.

The training configuration was as follows:
\begin{itemize}
    \item \textbf{Loss function:} Binary Cross-Entropy
    \item \textbf{Optimization:} Mini-batch Gradient Descent
    \item \textbf{Batch size:} 2048
    \item \textbf{Learning rate:} 0.001
    \item \textbf{Early stopping patience:} 5 epochs
    \item \textbf{Standardization:} Feature means and standard deviations computed using the training split only
\end{itemize}

This configuration resulted in stable and efficient convergence on the full dataset.

\subsection{Hero Recommendation System}

After training the neural network, I extended the project with a hero recommendation system designed to simulate draft decision-making.

The recommendation process operates as follows:
\begin{enumerate}
    \item The user inputs up to five friendly heroes and up to six enemy heroes.
    \item Typical per-hero statistics are generated using aggregate statistics computed from the dataset.
    \item For each hero not yet selected, the system simulates adding that hero to the friendly team as the sixth pick.
    \item Each simulated team composition is encoded into the same feature vector used during training.
    \item The trained neural network evaluates the predicted win probability for each candidate composition.
    \item The system returns the top three recommended heroes for each role: Tank, DPS, and Support.
\end{enumerate}

This recommendation system transforms the neural network from a passive predictor into an active decision-support tool, allowing users to explore how different hero picks influence match outcomes.

\subsection{SVM}
A Support Vector Machine (SVM) is a supervised machine learning method for classification which finds the hyperplane that divides the data into target classes while maximizing the margin from the boundary plane to the nearest points. As many datasets are not linearly separable in their original feature space, it is common practice to use a kernel function which implicitly maps the data into a higher dimension space in which the boundary is a hyperplane. The Radial Basis Function (RBF) kernel is a popular choice, mapping the data into an infinite-dimensional space and frequently enabling linear separation of the data. We began with this approach, but as we gathered more data the time to fit the model began to grow impractical (over 1 hour with 100,000 samples, implying a full day of training to fit to the final dataset). Therefore, we used the RBFSampler kernel approximation implemented in scikit-learn, which approximates a RBF feature map using random Fourier features as described in \cite{}

\subsection{Method 3}
Describe additional methods as needed.

% ----------------------------------------
% Behrouz Barati B's Methods (Tree-based + RAG)
% ----------------------------------------

\subsection{Random Forest}
Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of their predictions. Each tree is trained on a bootstrap sample of the data with a random subset of features at each split, reducing overfitting through decorrelation. The final prediction is:
\[
\hat{y} = \text{mode}\{h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_T(\mathbf{x})\}
\]
where $h_t(\mathbf{x})$ is the prediction of tree $t$ and $T=100$ is the number of trees.

\textbf{Configuration:} We used 100 estimators with default scikit-learn parameters. The model was trained on 105 features: 19 team statistics (kills, deaths, damage, healing, etc.) and 86 hero composition features (43 heroes $\times$ 2 teams encoded as binary presence indicators). Training used an 80/20 split with 5-fold stratified cross-validation.

\subsection{XGBoost}
XGBoost (Extreme Gradient Boosting) is a sequential boosting algorithm where each tree corrects errors from previous trees using second-order gradient optimization. It employs regularization (L1/L2) and column subsampling to prevent overfitting. The objective function minimized is:
\[
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{t=1}^{T} \Omega(f_t), \quad \Omega(f) = \gamma T + \frac{1}{2}\lambda \|w\|^2
\]
where $l$ is the logistic loss, $\Omega$ is the regularization term, $\gamma$ penalizes tree complexity, and $\lambda$ is L2 regularization on leaf weights $w$.

\textbf{Configuration:} We used 100 boosting rounds with learning rate 0.1, max depth 6, and 80\% column subsampling per tree. The objective function was binary logistic regression with evaluation metric set to AUC-ROC.

\subsection{CatBoost}
CatBoost (Category Boosting) extends gradient boosting with ordered boosting to reduce prediction shift and native handling of categorical features. It uses symmetric trees and applies L2 regularization on leaf values for stability. Ordered boosting computes unbiased gradients using a random permutation $\sigma$ of training examples:
\[
g_i = \frac{\partial L(y_i, F^{(\sigma, i-1)}(x_i))}{\partial F}, \quad F^{(\sigma, i-1)} = \sum_{j: \sigma(j) < \sigma(i)} f_j
\]
where gradients for example $i$ are computed using only preceding examples in the permutation.

\textbf{Configuration:} We used 100 iterations with default learning rate, depth 6, and L2 leaf regularization of 3. CatBoost's ordered boosting reduces overfitting without extensive hyperparameter tuning.

\subsection{Retrieval-Augmented Generation (RAG)}
We implemented a RAG pipeline to enable natural language exploration of match data. The system converts match records into text documents, embeds them using \texttt{mxbai-embed-large}, and stores vectors in ChromaDB with HNSW indexing. Document retrieval uses L2 (Euclidean) distance:
\[
d(q, d) = \|\mathbf{e}_q - \mathbf{e}_d\|_2 = \sqrt{\sum_{i=1}^{n}(e_{q,i} - e_{d,i})^2}
\]
where $\mathbf{e}_q$ and $\mathbf{e}_d$ are the embedding vectors for query $q$ and document $d$.

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Document Processing:} Each match is converted to a structured text document containing team compositions, performance statistics, and outcomes.
    \item \textbf{Meta Statistics:} Aggregate statistics (averages, hero win rates, damage/kill totals) are pre-computed and prioritized in prompts for accurate aggregate queries.
    \item \textbf{Retrieval:} On query, the top 100 similar documents are retrieved via L2 distance.
    \item \textbf{Generation:} Retrieved context is combined with meta statistics and passed to \texttt{llama3.1} for response generation.
\end{itemize}

\textbf{Winner Prediction:} The system also predicts match winners using direct LLM prompting on a 20-sample test subset. Match statistics (kills, deaths, damage, healing) are passed directly to \texttt{llama3.1} without semantic retrieval, achieving 80\% accuracy (16/20 correct).

% ============================================================================
\section{Evaluation}
% ============================================================================

This section evaluates the performance of the neural network model and the hero recommendation system.

\subsection{Evaluation Metrics}

The primary metric used to evaluate the neural network was \textbf{accuracy}, defined as the proportion of matches for which the predicted winner matched the true winner. Accuracy was chosen because the classification task is binary and balanced after applying randomized team flipping during data cleaning.

In addition to accuracy, qualitative evaluation was performed for the hero recommendation system by observing whether the predicted win probabilities aligned with known competitive patterns such as role balance, synergy, and meta compositions.

\subsection{Results}

The trained neural network achieved the following performance:
\begin{itemize}
    \item \textbf{Training Accuracy:} 0.915
    \item \textbf{Validation Accuracy:} 0.913
    \item \textbf{Test Accuracy:} 0.915
\end{itemize}

The close alignment between training, validation, and test accuracy indicates that the model generalizes well and does not overfit the dataset. Two factors were particularly important in achieving this stability: randomized team flipping, which removed positional bias, and the exclusion of final score information from the input features, which prevented target leakage.

For the hero recommendation system, qualitative testing showed that familiar meta compositions consistently resulted in higher predicted win probabilities, while role-imbalanced or anti-synergistic drafts produced lower probabilities. These observations matched intuitive expectations from competitive play and suggest that the neural network learned structural relationships such as role composition and synergy rather than relying solely on surface-level statistics. (KD)

\begin{table}[H]
    \centering
    \caption{Model Comparison (330,914 matches)}
    \label{tab:results}
    \begin{tabular}{lccc}
        \toprule
        Model & Accuracy & AUC-ROC & F1-Score \\
        \midrule
        Neural Network & \textbf{91.50\%} & -- & -- \\
        SVM & 86.00\% & -- & -- \\
        Random Forest & 89.76\% & 96.94\% & 89.59\% \\
        XGBoost & 89.97\% & 97.16\% & 89.81\% \\
        CatBoost & 89.88\% & 97.05\% & 89.72\% \\
        RAG (LLM) & 80.00\% & -- & -- \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{RAG Query Examples}
The RAG system successfully answers natural language queries about match statistics:
\begin{itemize}
    \item \textit{``What is the average match duration?''} $\rightarrow$ 673 seconds (11.2 minutes)
    \item \textit{``What is the average K/D ratio?''} $\rightarrow$ 2.57
    \item \textit{``What is the average damage dealt per match?''} $\rightarrow$ 173,503 total (86,751 per team)
\end{itemize}

\subsection{Discussion}
Discuss and analyze the results:
\begin{itemize}
    \item Which method performed best and why?
    \item Were there any surprising findings?
    \item What are the limitations of each method?
\end{itemize}

\textbf{Tree-Based Models:} All three ensemble methods achieved approximately 90\% accuracy with minimal performance differences. XGBoost achieved the highest metrics (89.97\% accuracy, 97.16\% AUC-ROC), while CatBoost trained fastest (1.76s). The high AUC-ROC scores ($>$97\%) indicate strong class separation capability. Feature importance analysis revealed that deaths, kills, and final hits were the most predictive features across all models.

\textbf{RAG System:} The RAG pipeline achieved 80\% accuracy on winner prediction (16/20 correct)---lower than tree-based methods but offering interpretable natural language responses. Its primary value lies in enabling non-technical exploration of match patterns rather than pure prediction accuracy.

% Figures (small size)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/model_comparison.png}
    \caption{Performance comparison of tree-based models.}
    \label{fig:model_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/combined_roc.png}
        \caption{ROC curves for tree-based models.}
        \label{fig:roc_curves}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/xgb_feature_importance.png}
        \caption{XGBoost feature importance.}
        \label{fig:feature_importance}
    \end{minipage}
\end{figure}

% ============================================================================
\section{Conclusion}
% ============================================================================

\subsection{Summary}
Summarize the key findings of your project. Restate the problem, the methods used, and the main results.

\subsection{Future Work}
Discuss potential improvements and future directions:
\begin{itemize}
    \item What could be done to improve the results?
    \item What additional experiments could be conducted?
    \item How could this work be extended?
\end{itemize}

% Manual references:
\begin{thebibliography}{9}

\bibitem{ref1}
Cordova, Chaim Joseph A., Carl Victor A. Villaceran, and Christine F. Peña. (2024). 
\textit{Predicting League of Legends Match Outcomes Through Machine Learning Models Using Past Match Player Performance}. 

\bibitem{ref2}
Author(s). (Year). \textit{Title of the paper}. Journal/Conference Name.

\bibitem{xiao2021wpgbdt}
H. Xiao, Y. Liu, D. Du, and Z. Lu. (2021). \textit{WP-GBDT: An Approach for Winner Prediction using Gradient Boosting Decision Tree}. IEEE International Conference on Big Data, pp. 5765--5769. DOI: 10.1109/BigData52589.2021.9671688

\bibitem{akhmedov2021dota2}
K. Akhmedov and A. Phan. (2021). \textit{Machine Learning Models for DOTA 2 Outcomes Prediction}. arXiv preprint arXiv:2105.09953.

\bibitem{gourdeau2020hero}
D. Gourdeau and L. Archambault. (2020). \textit{Discriminative Neural Network for Hero Selection in Professional Heroes of the Storm and DOTA 2}. IEEE Transactions on Games, vol. 12, no. 4, pp. 427--436. DOI: 10.1109/TG.2020.2972463

\bibitem{randomkitchensinks}
10.5555/2981780.2981944

\end{thebibliography}

\end{document}
